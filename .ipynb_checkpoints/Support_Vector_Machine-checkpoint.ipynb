{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Linear, non-linear classification, regression and outliers detection algo.\n",
    "* It not only classify the data but keep largest distance from points.\n",
    "* Decision tree learn axis parallel line, KNN learns vonavare diagram, SVM learn single line.\n",
    "* Web document classification\n",
    "    - High number of features(words)\n",
    "    - Low number of  labeled training  samples available\n",
    "    - Highly sparse data (Not all word in all documents)\n",
    "    - Boosted decision tree only works until around 4k features\n",
    "    - SVM works well even if low training data\n",
    "    - Training scales well with large number of features.\n",
    "![](images/SVM_error.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/SVM_line_eq.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Binary classification can be viewed as the task of separating classes in feature space\n",
    "![](images/SM_binary.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In decision tree we learn rules, in KNN we do not learn anything, naive bayes learn probability, here we are learning w and b, from data we learn best w and b.\n",
    "![](images/SVM_max_width.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We want to find line such that we can have max distance at both side, meaning maximize the width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Margin of a classifier\n",
    "* It is width that boundary could be increased by before hitting a data point.\n",
    "* Max margin linear classifier is the with max margin\n",
    "* It is simplest kind of SVM called Linear SVM\n",
    "* Imagine there is a road and we are going middle of the road, and we need max width of the road."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/SVM_margin.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Support vectors are those data points that the margin pushes up against. 2 white and 1 black support vectors in right margin image.\n",
    "* Training data may not comprising all possibilities, so there are a chance points very near to line easily misclassified so width of margin matters\n",
    "* Only support vectors are important, other training example are ignorable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/SVM_margin_optimization.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* After getting W and b, we can use `WX + b = 0` as boundary line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM for non linear\n",
    "* Data can not be linearly separable because of noise. Some data are by nature not linearly separable.\n",
    "* In such case above optimization will break. So we need some relaxed constraints.\n",
    "![](images/SVM_slack.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/SVM_non_linear.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We transformed original training samples onto a higher dimensional feature space\n",
    "* What is cost of the mapping?\n",
    "    - To learn quadratic function, number of features are (m+1) choose 2 = (m+1)(m)/2\n",
    "    - For degree of 'd' polynomials (d+m-1)choose d = O($m^d$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C parameter:\n",
    "* Constant attached to classification error\n",
    "$$Error = C * Classification Error + Margin Error$$\n",
    "* Having large C means, focus on classification error. Focusing on correctly classifying the points than finding margin.\n",
    "* Small C focus on large margin.\n",
    "![](images/svm_c.PNG)\n",
    "![](images/svm_c_1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Trick\n",
    "* Kernel is a function which helps us out to classify the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We already see, Linear SVM helps with linearly separable data.\n",
    "* Second was non linearly separable data, meaning linear data with noise, we used slack variable to relax our constrains. But still it was part of linear SVM technique\n",
    "* Third case was feature space mapping when data is truly non linear. But it is expensive. To overcome it we use kernel tricks.\n",
    "![](images/SVM_K_matrix.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using feature mapping method we get,\n",
    "![](images/SVM_kernel_1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using kernel trick,\n",
    "![](images/SVM_kernel_trick.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/SVM_kernel_vs_mapping.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Polynomial kernel\n",
    "![](images/svm_poly_kernel.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/SVM_linear_poily_kernel.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM kernel Functions\n",
    "* Polynomial Kernel\n",
    "$$K(a,b) = (a \\cdot b + 1)^d$$\n",
    "\n",
    "* Radial Basis kernel\n",
    "    - Useful to detect multiple circle boundaries\n",
    "$$K(a,b) = exp(- \\frac{(a-b)^2}{2 \\sigma^2})$$\n",
    "\n",
    "* Hyperbolic Tangent\n",
    "\n",
    "$$K(a,b) = tanh(\\kappa a \\cdot b - \\delta)$$\n",
    "\n",
    "* Both Radial Basis and Hyperbolic tangent project onto infinite dimensional space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using Kernel we can get higher dimensional space without loosing efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Parameter Tuning\n",
    "* Before tuning, make sure to\n",
    "    - Map categorical features to numerical values\n",
    "    - Re-scale features so that features with high value do not dominate.\n",
    "* Pick up RBF kernel\n",
    "* Use cross-validation to find best C and $\\sigma$ parameter values, Use the best C, $\\sigma$ to train on the entire training set.\n",
    "* Then Test.\n",
    "* We can plot, parameters vs accuracy graph too\n",
    "![](images/SVM_parameter_vs_accuracy.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Class imbalance:\n",
    "* Detecting fraud transaction, cancer detection. In such examples we have so many instances of one class, compared to others.\n",
    "* Using such data, default SVM formulation will learn highly skewed hyperplanes.\n",
    "* We want generalization error equally distributed between positive and negative class\n",
    "* One way is to up-sample minority class or down sample majority class. \n",
    "    - 10 case with cancer, 990 case no cancer, repeat 10 cancer case 99 times (up sampling) or pick 10 random non cancer instances (down sampling). We down sample when we have enough minority data, upsample when we have not enough minority data.\n",
    "    \n",
    "![](images/SVM_class_imbalance.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Formula trying to adjust large slack associated with major class. It try to normalize slack so for both class it is equally distributed.\n",
    "* N+ is say major class, Number of example in positive class\n",
    "* N- number of examples in minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantage of SVM\n",
    "* W tells important of features. if w is near to 0 for some feature, it is less important\n",
    "* Flexibility in choosing the similarity function (by choosing the kernel)\n",
    "* Exploits the sparseness of solution when dealing with large datasets.\n",
    "    - Only SVM specify the hyperplane irrespective of size of dataset.\n",
    "    - We have 100M instances, KNN will store entire data to make prediction at run time. SVM will store only coefficient for all feature and one extra b.\n",
    "* Can handle large feature sets efficiently\n",
    "    - Complexity does not depend on the dimensionality of feature space\n",
    "* Maximum margin generalizes better.\n",
    "* In decision tree we used greedy algo to reduce impurity from parents to child, solution may not be optimum. In SVM it is optimum. Convex optimization problem guaranteed to converge to global min. \n",
    "\n",
    "### Shortcomings\n",
    "* Sensitive to noise and outliers\n",
    "    - Some noisy or outlier training samples can significantly alter the hyperplane and hence the performance.\n",
    "    - Slack variable capture Euclidian distance from opposite side boundary. If outlier is far at other side, Squaring the distance make even worsen.\n",
    "* SVM does not provide posterior probability which is needed by many application (how confident the prediction is)\n",
    "* Formulation is not clean for multi-labeled classification\n",
    "    - For m classes, train m, one vs rest binary classifier\n",
    "        - We have 3 classes A,B,C we train A|BC, B|AC, C|AB, one classifier predict in A, other 2 in AC and AB. so we can predict that instance belongs to class A.\n",
    "    - Decision tree, naive and KNN all inherently handle multiclass classification\n",
    "    - Train m(m-1)/2 different binary classifiers on pairs of classes\n",
    "        - Classify test points based on which class receives highest votes\n",
    "        - For large m requires larger training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
