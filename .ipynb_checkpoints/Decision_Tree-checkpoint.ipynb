{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Given collection of labeled examples (x, f(x)), return function h that approximates f. h is a decision tree\n",
    "* Predicting tumor as benign or malignant\n",
    "* Classify credit card transaction as fraud or legitimate\n",
    "* Works for both classification and regression task.\n",
    "* Fundamental component for random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hunt's tree induction algorithm\n",
    "* $D_t$ be the set of training records that reach node t.\n",
    "* If $D_t$ contains records that belong the same class $y_t$, then t is a leaf node labeled as $y_t$. (Pure node)\n",
    "* If $D_t$ contains records that belong to more than 1 class, use an attribute test to split the data into smaller subsets. Recursively apply procedure to each subset.\n",
    "![](images/decision_tax.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Binary decision tree is better, Multi-way tree becomes bushy and when we have small dataset, there is a chance that each node gets few instances for training and may lead to inaccurate model. Also causes overfitting.\n",
    "* At impure node we do more splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to choose attribute and value of attribute and order of attribute for split?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tree should not tall, we should reach end as soon as possible that is our goal. We choose the attributes such that impurity reduction from parent to children is maximum.\n",
    "* Trying all combination  will cause combinatorial explosion.\n",
    "* We go with greedy algo, \n",
    "    - We only care about parent child relationship, we do not care any further level.\n",
    "    - In greedy strategy, we split the records based on an attribute test that optimize certain criterion of class impurity.\n",
    "* How to specify attribute test condition?\n",
    "    - Depends on attribute types: nominal vs continuous\n",
    "    - Depends on number of ways to split (2-way split, multi-way split)\n",
    "    - In multi way we use as many partition as distinct values, Ex. for car type, we divide in  3 part Family, Sports and Luxury\n",
    "    - For Binary split we divides values into 2 subsets, we need to find optimal partition. Ex. {sports, luxury} and Family or {Family, Luxury} and Sports. Go with max reduction.\n",
    "    - For continuous attributes, Discretization to form categorical attributes, We can fix discretization at the beginning.  Ranges can be found by equal interval bucketing, equal frequency bucketing or clustering\n",
    "    - Binary Decision can be chosen by choosing best cut(threshold) Ex. income less than 80K and more than 80K.\n",
    "* How to determine the best split?\n",
    "    - Before splitting we have 10 records of class 0 and 10 records of class 1.\n",
    "    ![](images/decision_tax_car_type.PNG)\n",
    "    - If we split by Own car, Both children are highly impure node, One node has 6 instance of one class and 4 of another, other node has 4 instance of 1 class and other has 6. This is not good, We can not reach to conclusion\n",
    "    - If we split by car type? middle node is pure, and other 2 nodes are mostly pure\n",
    "    - Splitting via unique attribute column is not good at all.\n",
    "    - In short nodes with homogeneous class distribution are preferred (pure node) (low degree of impurity)\n",
    "    - We measure impurity value and compare with parent, we need highest reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data[:, 2:] # Petal length and width\n",
    "y = iris.target\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2)\n",
    "tree_clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To visualize trained decision tree using `export_graphviz()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-01e30eef596c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m export_graphviz(tree_clf, out_file=image_path(\"iris_tree.dot\"), feature_names=iris.feature_names[2:], \n\u001b[0m\u001b[0;32m      2\u001b[0m                 class_names=iris.target_names, rounded=True, filled=True)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'image_path' is not defined"
     ]
    }
   ],
   "source": [
    "export_graphviz(tree_clf, out_file=image_path(\"iris_tree.dot\"), feature_names=iris.feature_names[2:], \n",
    "                class_names=iris.target_names, rounded=True, filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measures of node impurity\n",
    "\n",
    "### GINI index\n",
    "* GINI index at given node t\n",
    "$$GINI(t) = 1 - \\sum_j[p(j|t)^2]$$\n",
    "- p(j|t) is relative frequency of class j at node t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
