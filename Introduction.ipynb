{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Machine Learning(ML)?\n",
    "* Teaching computers to learn to perform task from past experience(data).\n",
    "* A computer program is said to learn from experience E, with respect to some task T and performance measure P. It's performance P for task T, improves with experience E.\n",
    "\n",
    "### Application of ML\n",
    "* email spam filtering\n",
    "* Text and voice recognition\n",
    "* Web search engine (Ranking)\n",
    "* Self-driving cars.\n",
    "* Photo tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Types of ML</center></h1>\n",
    "\n",
    "## Supervised Learning\n",
    "- Labeled data\n",
    "- Direct feedback\n",
    "- Predict outcome/feature\n",
    "- Learn model from labeled training data that allows to make prediction of future unseen data.\n",
    "![](images/supervised.PNG)\n",
    "\n",
    "### Classification\n",
    "* Discrete label (spam-non spam, benign-malign)\n",
    "* Example:\n",
    "    - Email spam filtering\n",
    "    - Hand writing digit recognition\n",
    "    - \n",
    "\n",
    "### Regression\n",
    "* Predict continuous value.\n",
    "* We are given a predictor(Explanatory) variables and a continuous response (target/outcome) variable, we try to find relationship between that.\n",
    "* Example\n",
    "    - Predicting SAT score of students (time spent studying, mock exam score)\n",
    "    - Finding price of house given set of features (predictors)\n",
    "* We want to minimize the distance (avg squared distance) between sample points and fitted line, and we can use line's intercept and slope to predict future data.\n",
    "\n",
    "### Common Algorithm\n",
    "\n",
    "##### K-nearest neighbors\n",
    "\n",
    "##### Linear Regression\n",
    "\n",
    "##### Logistic Regression\n",
    "* Can be used for classification.\n",
    "* It can output value that corresponds probability of belonging to a given task.\n",
    "\n",
    "##### Support vector machine (SVM)\n",
    "\n",
    "##### Decision tree and random forest\n",
    "\n",
    "##### Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning\n",
    "- No labels\n",
    "- No feedback\n",
    "- Find hidden structure in data\n",
    "\n",
    "### Clustering\n",
    "- Organize pile of info into meaningful subgroups (clusters)\n",
    "- Each group share certain degree of similarities.\n",
    "- Example\n",
    "    - Discovering customer group\n",
    "![](images/clustring.PNG)\n",
    "\n",
    "### Common clustering algorithm\n",
    "\n",
    "##### K-means\n",
    "\n",
    "##### Hierarchical cluster analysis (HCA)\n",
    "* Subdivide group into smaller groups\n",
    "\n",
    "##### Expectation maximization\n",
    "\n",
    "### Dimensionality reduction\n",
    "* Often data is in high dimension- occupy storage and CPU\n",
    "* We want to compress data to smaller dimensional subspace while retaining most of relevant info.\n",
    "![](images/dimenstionality_reduction.PNG)\n",
    "\n",
    "### Visualization and dimensionality reduction algorithm\n",
    "\n",
    "##### Principal component analysis (PCA)\n",
    "\n",
    "##### Kernel PCA\n",
    "\n",
    "##### Locally linear embedding (LLE)\n",
    "\n",
    "##### t-distributed stochastic neighbor embedding (t-SNE)\n",
    "\n",
    "### Anomaly detection\n",
    "* Detecting abnormal and unusual case\n",
    "* Detect unusual credit card transaction\n",
    "* Automatically removing outlier from dataset.\n",
    "\n",
    "### Association rule mining\n",
    "* Discover interesting relationship among attributes.\n",
    "### Associated rule learning algorithms\n",
    "\n",
    "##### Apriori\n",
    "##### Eclat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semisupervised Learning\n",
    "* Partially labeled data.\n",
    "* Algorithms\n",
    "    - Deep belief networks\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "* Agent observe environment, select and perform an action, and get reward in return. It learn itself to get most reward over the time.\n",
    "- Decision process\n",
    "- Reward system\n",
    "- Learn series of action\n",
    "- Improves performance via interaction with environment.\n",
    "- Here the goal is to maximize reward given by reward function\n",
    "- Example\n",
    "    - Chess engine (reward is win or lose.)\n",
    "![](images/reinforcement.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Process\n",
    "![](images/process.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Selected features should be in same scale. Transform to range [0,1] or a standard normal distribution with mean 0 and unit variance.\n",
    "* Some features which are highly correlated, so redundant at certain degree. Car's mileage is correlated with age.\n",
    "* Randomly divide dataset in training and testing data.\n",
    "* Train different model, compare their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Hyperparameter** are parameters that are not  learned from data but represent the knobs of a model that we can turn to improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Applying ML techniques to dig into large amount of data can help discover patterns that were not immediately apparent. It is called **Data mining**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sequence mining is predicting next events, click streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supervised vs unsupervised\n",
    "* Labeled vs unlabeled\n",
    "* output is known vs not known\n",
    "* More measures for accuracy vs not enough\n",
    "* Controlled environment vs not controlled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Learning (Offline learning)\n",
    "* Incapable of learning incrementally. Must be trained with all available data.\n",
    "* System is trained and launched in production, after that system will not learn anymore.\n",
    "* Requires great compute power\n",
    "\n",
    "### Online Learning\n",
    "* Train system incrementally by feeding it data instances sequentially.\n",
    "* Learn from new data on the fly\n",
    "* Can work with restricted resources\n",
    "* Useful when data is huge and can not load in memory at same time. We train model with part of data.\n",
    "* If learning rate is high, system will adapt new data fast and forget about old data.\n",
    "* Slow learning rate will less sensitive to new noisy data (unrepresentative data)\n",
    "* We have to monitor performance degradation due to bad data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance based Learning\n",
    "* Mark all email spam identical to or similar to known spam emails. \n",
    "* This requires measure of similarity between two emails.\n",
    "* System learns example by heart and generalized to new cases using similarity measure.\n",
    "![](images/instance.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model based learning\n",
    "* Create model using example, using model predict outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges\n",
    "\n",
    "#### Insufficient quantity of training data\n",
    "\n",
    "#### Non representative training data\n",
    "* Training data has to be representative of new cases we want to generalize to.\n",
    "* If sample is too small there will be a sampling noise, if sample is too large sample can be non representative if the sampling method is flawed. It is called sampling bias.\n",
    "\n",
    "#### Poor Quality Data\n",
    "* Data with errors, outlier and noise\n",
    "* Fix outliers\n",
    "* Fill missing value, ignore that attribute, train one model with missing value feature or one without it.\n",
    "\n",
    "#### Irrelevant feature\n",
    "* Feature selection: Select most useful features.\n",
    "* Feature extraction: Combining existing features to produce more useful one (dimensionality reduction can help)\n",
    "* Create new feature by gathering new data.\n",
    "\n",
    "#### Overfitting of training data\n",
    "* Model perform well on training data, but do not generalize well.\n",
    "* Overfitting happens when model is too complex relative to amount and noisiness of training data\n",
    "    - Simplify model by choosing fewer parameters, use linear than high degree polynomial\n",
    "    - Gather more training data\n",
    "    - Reduce noise in training data (fix data error, remove outliers)\n",
    "    \n",
    "* Constraining model to make it simpler and reduce risk of overfitting is called **regularization**. Keep balance between fitting data perfect and keeping model simple enough to generalize well.\n",
    "    - $\\theta_0$ and $\\theta_1$ are  2 parameter in linear model. Which gives 2 degree of freedom to a model. If we force $\\theta_1$ = 0, only 1 degree of freedom. Harder to fit data well. We only can move line up or down. So end up around mean.\n",
    "    - We want to find right balance between fitting the data and keeping model simple enough.\n",
    "    - Regularization can be controlled by hyperparameter.\n",
    "\n",
    "#### Under fitting training data\n",
    "* Model is too simple to lean underlying structure of data.\n",
    "* To overcome,\n",
    "    - Select more powerful model\n",
    "    - Feeding better features to the learning algo\n",
    "    - Reducing contrains on the model(Reduce regularization of hyper parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sequence of data processing is called **data pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance Measure (norms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Vector norms is total size or length of all vector in vector space or matrix.\n",
    "\n",
    "#### L0 Norm\n",
    "* Total number of non zero elements in a vector.\n",
    "* L0 norm of (0,0) and (0,2) is 1.\n",
    "\n",
    "\n",
    "#### L1 norm\n",
    "* MAE (Mean absolute error)(mean absolute distance) corresponds to l1 norm. Also known as Manhattan norm(Taxicab norm), as it measure distance between 2 points in a city if you can travel along orthogonal city block.\n",
    "* Sum of magnitudes of the vector in a space.\n",
    "* All component of the vector are weighted equally.\n",
    "* For (3,4), L1 norm is |3| + |4| = 7\n",
    "* In image we can see that taxicab travels between Manhattan blocks from (0,0) to (3,4).\n",
    "![](images/l1.jpg)\n",
    "\n",
    "#### L2 norm\n",
    "* Euclidian norm. Shortest distance from 1 point to other.\n",
    "* RMSE(Root mean square error)\n",
    "* L2 norm for (3,4) = $\\sqrt{|3|^2 + |4|^2}$ = 5\n",
    "![](images/l2.jpg)\n",
    "* Each component of vector is squared, so outlier has more weight so it can skew the results.\n",
    "\n",
    "#### Lk norm\n",
    "* Lk norm of (3,4) = $\\sqrt[k]{|3|^k + |4|^k}$\n",
    "\n",
    "#### L$\\infty$ norm\n",
    "* Max absolute value in vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Higher the norm index more it focus on large value, neglect small values. SO, RMSE is more sensitive to outlier than MAE. When Outlier are exponentially rare (bell shape curve) RMSE performs well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
